---
title: "Final"
subtitle: "Eli Dewald"
date: "May 27, 2025"
format: html
toc: true
---
[GitHub Repository](https://github.com/eliasdewald/ENVS-193DS_spring-2025_final)
```{r}
#| message: false # make messages not show
#| warning: false # make warnings not show

library(tidyverse) # general use
library(here) # file organization
library(MuMIn) # choosing best model
library(janitor) # cleaning data frames
library(DHARMa) # running model diagnostics
library(lubridate) # cleaning up dates

sst <- read.csv( # read in data
  here("data", "SST_update2023.csv")) # set file path
```

# Problem 1. Research Writing 

## a. 

In part 1, they conducted a Pearson's correlation coefficient test to study the correlation between the two variables. In part 2, they conducted an ANOVA test to study whether there were significant differences in means between more than two sources.

## b. 

A Tukey's multiple comparisons of mean test could be used to find Tukey's HSD, which would be useful for discerning which sources have significant differences between them in average nitrogen load (kg year-1). Running an Effect Size for an ANOVA test could be used to find η~2~, which would be useful to study how much source explains average nitrogen load (kg year-1). 

## c. 

We found that there is a [blank] (r = correlation coefficient) correlation between distance from headwater (km) and annual total nitrogen load (kg year-1) (Pearson' R test, t(degrees of freedom) = t-statistic, p = 0.03, α = significance level). 

We found a [blank] (η~2~ = effect size) difference between sources (urban land, atmospheric deposition, fertilizer, wastewater treatment, and grasslands) in nitrogen load (kg year-1) (one-way ANOVA, f(among groups degrees of freedom, within groups degrees of freedom) = f-statistic, p = 0.02, α = significance level). On average, [blank] sources tended to have [blank] nitrogen load than [blank, etc.] (Tukey HSD, 95% CI:[lower bound, upper bound] (kg year-1)) sources. 

# Problem 2. Data visualization

## a. 

```{r}
#| message: false
sst_clean <- sst |> # create new object from sst
  mutate(date = as_date(date),
         year = year (date),
         month = month(date)) |>  # make sure date is a date
  filter(date > as_date("2017-12-31")) |> 
  # filter to only includes dates from 2018 and after
  select(year, month, temp) |>  # select columns of interest
  group_by(year, month) |> # group by month and year
  summarize(mean_monthly_sst = mean(temp, na.rm = TRUE)) |> 
  # calculate monthly mean sst
  mutate(year = as_factor(year), # make sure year is a factor
         year = fct_relevel(year, # change year factor levels to YYYY format
            "2018", "2019", "2020", "2021", "2022", "2023")) |> 
  mutate(month = case_when( # rename values
    month == "1" ~ "Jan", # change "1" to "Jan"
    month == "2" ~ "Feb", # change "2" to "Feb"
    month == "3" ~ "Mar", # change "3" to "Mar"
    month == "4" ~ "Apr", # change "4" to "Apr"
    month == "5" ~ "May", # change "5" to "May"
    month == "6" ~ "Jun", # change "6" to "Jun"
    month == "7" ~ "Jul", # change "7" to "Jul"
    month == "8" ~ "Aug", # change "8" to "Aug"
    month == "9" ~ "Sep", # change "9" to "Sep"
    month == "10" ~ "Oct", # change "10" to "Oct"
    month == "11" ~ "Nov", # change "11" to "Nov"
    month == "12" ~ "Dec"), # change "12" to "Dec"
    month = ordered(month),
    month = fct_relevel(month,
            "Jan", "Feb", "Mar", "Apr", "May", "Jun", 
            "Jul", "Aug", "Sep", "Oct", "Nov", "Dec")) |>
  ungroup() # ungroup data frame
slice_sample(sst_clean, # slice sst_clean data frame
             n = 5) # display 5 rows
str(sst_clean) # display structure of sst_clean data frame
```

